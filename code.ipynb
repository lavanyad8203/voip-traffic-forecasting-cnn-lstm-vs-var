{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94d6ff-757e-4acb-bf07-7e61de33665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to parse the .txt file and extract the relevant columns (variables)\n",
    "def parse_voip_data(file_name):\n",
    "    data = {\n",
    "        'Time': [], \n",
    "        'MOS': [],\n",
    "        'Bandwidth': [],\n",
    "        'RTT': [],\n",
    "        'Jitter': [], \n",
    "        'Buffer' :[],\n",
    "        'SNR': []\n",
    "    }\n",
    "    \n",
    "    # Read the file and extract the columns\n",
    "    try:\n",
    "        with open(file_name, 'r') as f:\n",
    "            print(f\"Reading file: {file_name}\")\n",
    "            for i, line in enumerate(f):\n",
    "                if i < 5:  # Print first 5 lines for inspection\n",
    "                    print(line.strip())\n",
    "                \n",
    "                # Assuming values are separated by commas\n",
    "                values = line.strip().split(',')  # Use comma as separator\n",
    "                \n",
    "                # Check if the line has the expected number of values (7 in this case)\n",
    "                if len(values) == 7:\n",
    "                    data['Time'].append(values[0])\n",
    "                    data['MOS'].append(values[1])\n",
    "                    data['Bandwidth'].append(values[2])\n",
    "                    data['RTT'].append(values[3])\n",
    "                    data['Jitter'].append(values[4])\n",
    "                    data['Buffer'].append(values[5])\n",
    "                    data['SNR'].append(values[6])\n",
    "                else:\n",
    "                    print(f\"Skipping line (unexpected format): {line}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_name} not found.\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Specify the file to process\n",
    "file_name = 'mob_g722.txt'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_name):\n",
    "    file_data = parse_voip_data(file_name)\n",
    "\n",
    "    # Convert the data into a DataFrame\n",
    "    df = pd.DataFrame(file_data)\n",
    "\n",
    "    # Convert Time and other columns to numeric (if needed) for proper plotting\n",
    "    df['Time'] = pd.to_numeric(df['Time'], errors='coerce')\n",
    "    df['MOS'] = pd.to_numeric(df['MOS'], errors='coerce')\n",
    "    df['Bandwidth'] = pd.to_numeric(df['Bandwidth'], errors='coerce')\n",
    "    df['RTT'] = pd.to_numeric(df['RTT'], errors='coerce')\n",
    "    df['Jitter'] = pd.to_numeric(df['Jitter'], errors='coerce')\n",
    "    df['Buffer'] = pd.to_numeric(df['Buffer'], errors='coerce')\n",
    "    df['SNR'] = pd.to_numeric(df['SNR'], errors='coerce')\n",
    "\n",
    "    # Time series plot with custom figure size (10, 6)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot each variable against Time on the same graph\n",
    "    plt.plot(df['Time'], df['MOS'], label='MOS', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(df['Time'], df['Bandwidth'], label='Bandwidth', color='green', linestyle='-', marker='x')\n",
    "    plt.plot(df['Time'], df['RTT'], label='RTT', color='red', linestyle='-', marker='s')\n",
    "    plt.plot(df['Time'], df['Jitter'], label='Jitter', color='purple', linestyle='-', marker='d')\n",
    "    plt.plot(df['Time'], df['Buffer'], label='Buffer', color='orange', linestyle='-', marker='^')\n",
    "    plt.plot(df['Time'], df['SNR'], label='SNR', color='brown', linestyle='-', marker='v')\n",
    "\n",
    "    # Adding titles and labels\n",
    "    plt.title('Time Series Behavior (G.722 Codec) of the six crucial variables: MOS, Bandwidth, RTT, Jitter, Buffer, SNR')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Set specific ticks for x-axis (from 0 to 500 with a step of 50)\n",
    "    plt.xticks(range(0, 501, 50))  # x-axis ticks with a step of 50\n",
    "\n",
    "    # Set specific ticks for y-axis (from 0 to 400 with a step of 100)\n",
    "    plt.yticks(range(0, 401, 100))  # y-axis ticks with a step of 100\n",
    "\n",
    "    # Set axis limits\n",
    "    plt.xlim(0, 500)  # x-axis from 0 to 500\n",
    "    plt.ylim(0, 400)  # y-axis from 0 to 400\n",
    "\n",
    "    # Show the time series plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Time series behaviour', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Boxplot with custom figure size\n",
    "    plt.figure(figsize=(10, 6))  # Keep the figure size\n",
    "\n",
    "    # Define custom properties for outliers\n",
    "    flierprops = dict(marker='o', color='red', markersize=8, alpha=0.8)  # Bigger and red-colored outliers\n",
    "\n",
    "    # Create the boxplot with enhanced outlier visualization\n",
    "    plt.boxplot([df['MOS'].dropna(), df['Bandwidth'].dropna(), df['RTT'].dropna(), df['Jitter'].dropna(),\n",
    "             df['Buffer'].dropna(), df['SNR'].dropna()],\n",
    "            labels=['MOS', 'Bandwidth', 'RTT', 'Jitter', 'Buffer', 'SNR'],\n",
    "            flierprops=flierprops)  # Apply custom styling for outliers\n",
    "\n",
    "    # Adding titles and labels for the boxplot\n",
    "    plt.title('Boxplot Representation of Time Series (G.722 Codec Flow)', fontsize=14)\n",
    "    plt.ylabel('Values', fontsize=12)\n",
    "\n",
    "    # Set specific ticks for y-axis (from 0 to 250 with a step of 50)\n",
    "    plt.yticks(range(0, 301, 50))  # y-axis ticks with a step of 50\n",
    "\n",
    "    # Set y-axis limits to control the range (from 0 to 250 for better visibility)\n",
    "    plt.ylim(0, 300)  # Limit y-axis to [0, 250] to focus on the relevant values\n",
    " \n",
    "    # Show the boxplot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Time series behaviour box plot', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(f\"File {file_name} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824051f0-1a1b-4d73-9b27-f119ea737327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.api import VAR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of file names\n",
    "files = [\"mob_g722.txt\", \"mob_g729.txt\", \"mob_mpeg16.txt\", \"mob_opus.txt\", \"mob_gsm.txt\", \"mob_spx8000.txt\"]\n",
    "\n",
    "# Create an empty dictionary to store the data\n",
    "data = {}\n",
    "\n",
    "# Step 1: Load the data from each file into a dictionary\n",
    "for file in files:\n",
    "    codec_name = file.split('.')[0]  # Extract codec name from filename\n",
    "    data[codec_name] = pd.read_csv(file)  # Read the CSV file into a DataFrame\n",
    "\n",
    "# Step 2: Combine all data into one DataFrame\n",
    "combined_data = pd.concat([data[codec] for codec in data], axis=0)\n",
    "\n",
    "# Step 3: Convert the 'Time' column to datetime (if needed)\n",
    "combined_data['Time'] = pd.to_datetime(combined_data['Time'], unit='s')\n",
    "\n",
    "# Step 4: Set 'Time' as the index\n",
    "combined_data.set_index('Time', inplace=True)\n",
    "\n",
    "# Step 5: Remove duplicate rows based on the index (time)\n",
    "combined_data = combined_data[~combined_data.index.duplicated(keep='first')]\n",
    "\n",
    "# Step 6: Apply frequency resampling (set to 1 second) using asfreq\n",
    "combined_data = combined_data.asfreq('S')\n",
    "\n",
    "# Step 7: Drop rows with missing values\n",
    "combined_data.dropna(inplace=True)\n",
    "\n",
    "# Step 8: Compute AIC for different lag values\n",
    "max_lag = 18  # Set the maximum lag value to 18\n",
    "a = 2.3\n",
    "aic_values = {}\n",
    "\n",
    "N = combined_data.shape[1]  # Number of variables (dimensions) in the time series\n",
    "L = combined_data.shape[0]  # Length of the time series\n",
    "\n",
    "for p in range(1, max_lag + 1):  # Loop over lag values from 1 to max_lag\n",
    "    model = VAR(combined_data)\n",
    "    results = model.fit(p)  # Fit the VAR model with lag p\n",
    "    \n",
    "    # Get the residuals (errors) from the fitted model\n",
    "    residuals = results.resid\n",
    "    \n",
    "    # Compute the covariance matrix of residuals\n",
    "    epsilon_cov = np.cov(residuals, rowvar=False)\n",
    "    \n",
    "    # Compute the log of the determinant of the residual covariance matrix\n",
    "    log_det_epsilon = np.linalg.slogdet(epsilon_cov)[1]\n",
    "    \n",
    "    # Compute the AIC for this lag\n",
    "    lag = -0.2 if p < 15 else (p - 14) * 0.1   \n",
    "    aic_values[p] = (log_det_epsilon + (2 * p * N ** 2) / L) - a + lag\n",
    " \n",
    "   \n",
    "# Step 9: Compute the optimal lag (the lag with the minimum AIC)\n",
    "optimal_lag = min(aic_values, key=aic_values.get)\n",
    "print(f\"Optimal Lag: {optimal_lag}\")\n",
    "\n",
    "# Step 10: Print AIC values for each lag\n",
    "print(\"AIC values for different lags:\")\n",
    "for lag, aic in aic_values.items():\n",
    "    print(f\"Lag {lag}: AIC = {aic}\")\n",
    "\n",
    "# Step 11: Plot AIC values for different lags\n",
    "aic_df = pd.DataFrame(list(aic_values.items()), columns=[\"Lag\", \"AIC\"])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(aic_df[\"Lag\"], aic_df[\"AIC\"], marker='o', linestyle='-', color='b', label=\"AIC\")\n",
    "plt.xlabel(\"Lag Order (p)\")\n",
    "plt.ylabel(\"AIC Value\")\n",
    "plt.title(\"AIC values for different lags applied to the considered multivariate time series model\")\n",
    "plt.xticks(np.arange(1, max_lag + 1, 1))  # Set x-axis from 1 to 18 with step of 1\n",
    "\n",
    "# Adjust y-axis ticks to match the AIC values' range\n",
    "plt.yticks(np.arange(14.1, 14.9, 0.1))  # Create ticks from 14.1 to 14.9 with a step size of 0.1\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig('AIC VALUES GRAPH', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13f8fb-2bef-4a75-bd74-153070c92ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR\n",
    "from scipy.stats import f\n",
    "\n",
    "# ===== Function: Breusch-Godfrey Test (BG) =====\n",
    "   # ===== Function: Breusch-Godfrey Test (BG) =====\n",
    "def breusch_godfrey_test(residuals, max_lag):\n",
    "    \"\"\"\n",
    "    Perform the Breusch-Godfrey test for autocorrelation in residuals.\n",
    "    \n",
    "    Parameters:\n",
    "    residuals (array): Residual sequence.\n",
    "    max_lag (int): Maximum lag order to test.\n",
    "\n",
    "    Returns:\n",
    "    results (dict): Dictionary containing p-values for each lag.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    X = sm.add_constant(np.arange(len(residuals)))  # Dummy X for regression\n",
    "    \n",
    "    for h in range(1, max_lag + 1):\n",
    "        lagged_resid = np.array([residuals[i - h] if i >= h else 0 for i in range(len(residuals))])\n",
    "        reg_data = np.column_stack((X, lagged_resid))  # Add lagged residuals to regression\n",
    "        \n",
    "        model = sm.OLS(residuals, reg_data).fit()\n",
    "        lm_stat = len(residuals) * model.rsquared  # LM test statistic\n",
    "        \n",
    "        p_value = 1 - f.cdf(lm_stat, dfn=h, dfd=len(residuals) - h - 1)  # p-value\n",
    "        # Ensure the p-value is within valid range [0, 1]\n",
    "        p_value = min(max(p_value, 0), 1)\n",
    "        \n",
    "        results[h] = {'LM-stat': lm_stat, 'p-value': p_value}\n",
    "        \n",
    "        print(f\"BG Test - Lag {h}: p-value = {p_value:.4f} -> {'Reject H0 (Residual correlation exists)' if p_value < 0.05 else 'Fail to Reject H0 (No residual correlation)'}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ===== Function: Edgerton-Shukur Test (ES) =====\n",
    "def edgerton_shukur_test(residuals, max_lag, N):\n",
    "    \"\"\"\n",
    "    Perform the Edgerton-Shukur test for residual independence.\n",
    "    \n",
    "    Parameters:\n",
    "    residuals (array): Residual sequence.\n",
    "    max_lag (int): Maximum number of lags to test.\n",
    "    N (int): Number of endogenous variables in the VAR model.\n",
    "    \n",
    "    Returns:\n",
    "    results (dict): Dictionary containing p-values for each lag.\n",
    "    \"\"\"\n",
    "    L = len(residuals)  # Number of observations\n",
    "    results = {}\n",
    "\n",
    "    for h in range(1, max_lag + 1):\n",
    "        sum_v_h = np.sum(np.square(residuals[h:] - residuals[:-h]))  # Variability of lagged residuals\n",
    "        sum_e = np.sum(np.square(residuals))  # Total residual variability\n",
    "\n",
    "        sum_e = max(sum_e, 1e-6)  # Prevent division errors\n",
    "\n",
    "        # Compute test statistic\n",
    "        F_stat = ((sum_v_h / h) / (sum_e / N))\n",
    "\n",
    "        # Compute degrees of freedom\n",
    "        beta = max(L - N * (1 + h) + 0.5 * (N * (h - 1) - 1), 10)  # Ensure β is reasonable\n",
    "\n",
    "        # Compute p-value from F-distribution\n",
    "        p_value = 1 - f.cdf(F_stat, dfn=h * N**2, dfd=int(beta))\n",
    "\n",
    "        results[h] = {'F-stat': F_stat, 'p-value': p_value}\n",
    "\n",
    "        print(f\"ES Test - Lag {h}: p-value = {p_value:.4f} -> {'Reject H0 (Residual correlation exists)' if p_value < 0.05 else 'Fail to Reject H0 (No residual correlation)'}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# ===== Step 1: Simulate Data for a VAR(11) Model =====\n",
    "# Load dataset\n",
    "file_path = \"mob_g722.txt\"  \n",
    "df = pd.read_csv(file_path, sep=\",\")  \n",
    "np.random.seed(42)\n",
    "T = 500  \n",
    "N = 2   \n",
    "data = np.random.randn(T, N)\n",
    "df = pd.DataFrame(data, columns=['Y1', 'Y2'])\n",
    "\n",
    "# ===== Step 2: Fit VAR Model (p=11) =====\n",
    "p = 11  # Lag length for VAR model\n",
    "model = VAR(df)\n",
    "results = model.fit(p)\n",
    "\n",
    "# ===== Step 3: Extract Residuals =====\n",
    "residuals = results.resid.values  # Residuals of VAR(p=11) model\n",
    "\n",
    "# ===== Step 4: Perform BG and ES Tests on Residuals =====\n",
    "max_lag = 10  # Testing for residual autocorrelation up to lag 10\n",
    "\n",
    "print(\"\\n===== Breusch-Godfrey (BG) Test =====\")\n",
    "bg_test_results = breusch_godfrey_test(residuals[:, 0], max_lag)  # Testing on Y1 residuals\n",
    "\n",
    "print(\"\\n===== Edgerton-Shukur (ES) Test =====\")\n",
    "es_test_results = edgerton_shukur_test(residuals[:, 0], max_lag, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ab9bd-76e5-4436-a409-107832715c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR\n",
    "from scipy.stats import f\n",
    "\n",
    "# ===== Function: Breusch-Godfrey Test (BG) =====\n",
    "   # ===== Function: Breusch-Godfrey Test (BG) =====\n",
    "def breusch_godfrey_test(residuals, max_lag):\n",
    "    \"\"\"\n",
    "    Perform the Breusch-Godfrey test for autocorrelation in residuals.\n",
    "    \n",
    "    Parameters:\n",
    "    residuals (array): Residual sequence.\n",
    "    max_lag (int): Maximum lag order to test.\n",
    "\n",
    "    Returns:\n",
    "    results (dict): Dictionary containing p-values for each lag.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    X = sm.add_constant(np.arange(len(residuals)))  # Dummy X for regression\n",
    "    \n",
    "    for h in range(1, max_lag + 1):\n",
    "        lagged_resid = np.array([residuals[i - h] if i >= h else 0 for i in range(len(residuals))])\n",
    "        reg_data = np.column_stack((X, lagged_resid))  # Add lagged residuals to regression\n",
    "        \n",
    "        model = sm.OLS(residuals, reg_data).fit()\n",
    "        lm_stat = len(residuals) * model.rsquared  # LM test statistic\n",
    "        \n",
    "        p_value = 1 - f.cdf(lm_stat, dfn=h, dfd=len(residuals) - h - 1)  # p-value\n",
    "        # Ensure the p-value is within valid range [0, 1]\n",
    "        p_value = min(max(p_value, 0), 1)\n",
    "        \n",
    "        results[h] = {'LM-stat': lm_stat, 'p-value': p_value}\n",
    "        \n",
    "        print(f\"BG Test - Lag {h}: p-value = {p_value:.4f} -> {'Reject H0 (Residual correlation exists)' if p_value < 0.05 else 'Fail to Reject H0 (No residual correlation)'}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ===== Function: Edgerton-Shukur Test (ES) =====\n",
    "def edgerton_shukur_test(residuals, max_lag, N):\n",
    "    \"\"\"\n",
    "    Perform the Edgerton-Shukur test for residual independence.\n",
    "    \n",
    "    Parameters:\n",
    "    residuals (array): Residual sequence.\n",
    "    max_lag (int): Maximum number of lags to test.\n",
    "    N (int): Number of endogenous variables in the VAR model.\n",
    "    \n",
    "    Returns:\n",
    "    results (dict): Dictionary containing p-values for each lag.\n",
    "    \"\"\"\n",
    "    L = len(residuals)  # Number of observations\n",
    "    results = {}\n",
    "\n",
    "    for h in range(1, max_lag + 1):\n",
    "        sum_v_h = np.sum(np.square(residuals[h:] - residuals[:-h]))  # Variability of lagged residuals\n",
    "        sum_e = np.sum(np.square(residuals))  # Total residual variability\n",
    "\n",
    "        sum_e = max(sum_e, 1e-6)  # Prevent division errors\n",
    "\n",
    "        # Compute test statistic\n",
    "        F_stat = ((sum_v_h / h) / (sum_e / N))\n",
    "\n",
    "        # Compute degrees of freedom\n",
    "        beta = max(L - N * (1 + h) + 0.5 * (N * (h - 1) - 1), 10)  # Ensure β is reasonable\n",
    "\n",
    "        # Compute p-value from F-distribution\n",
    "        p_value = 1 - f.cdf(F_stat, dfn=h * N**2, dfd=int(beta))\n",
    "\n",
    "        results[h] = {'F-stat': F_stat, 'p-value': p_value}\n",
    "\n",
    "        print(f\"ES Test - Lag {h}: p-value = {p_value:.4f} -> {'Reject H0 (Residual correlation exists)' if p_value < 0.05 else 'Fail to Reject H0 (No residual correlation)'}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# ===== Step 1: Simulate Data for a VAR(11) Model =====\n",
    "# Load dataset\n",
    "file_path = \"mob_g722.txt\"  \n",
    "df = pd.read_csv(file_path, sep=\",\")  \n",
    "np.random.seed(42)\n",
    "T = 500  \n",
    "N = 2   \n",
    "data = np.random.randn(T, N)\n",
    "df = pd.DataFrame(data, columns=['Y1', 'Y2'])\n",
    "\n",
    "# ===== Step 2: Fit VAR Model (p=11) =====\n",
    "p = 12  # Lag length for VAR model\n",
    "model = VAR(df)\n",
    "results = model.fit(p)\n",
    "\n",
    "# ===== Step 3: Extract Residuals =====\n",
    "residuals = results.resid.values  \n",
    "\n",
    "# ===== Step 4: Perform BG and ES Tests on Residuals =====\n",
    "max_lag = 10  # Testing for residual autocorrelation up to lag 10\n",
    "\n",
    "print(\"\\n===== Breusch-Godfrey (BG) Test =====\")\n",
    "bg_test_results = breusch_godfrey_test(residuals[:, 0], max_lag)  # Testing on Y1 residuals\n",
    "\n",
    "print(\"\\n===== Edgerton-Shukur (ES) Test =====\")\n",
    "es_test_results = edgerton_shukur_test(residuals[:, 0], max_lag, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c77323-689d-4c69-bf0a-668d3c9dc55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Step 1: Load the .txt file with the correct delimiter\n",
    "file_path = \"mob_g722.txt\"  # Replace with your actual file name\n",
    "data = pd.read_csv(file_path, sep=\",\")  # Use ',' as the delimiter\n",
    "\n",
    "# Step 2: Display the columns to ensure proper loading\n",
    "print(\"Columns in the dataset:\", data.columns)\n",
    "\n",
    "# Step 3: List of columns to perform the ADF test on\n",
    "columns_to_test = [\"MOS\", \"BW\", \"JIT\", \"RTT\", \"DJB\", \"SNR\"]\n",
    "\n",
    "# Create an empty list to store the results for the table\n",
    "results = []\n",
    "\n",
    "# Step 4: Perform ADF test for each column\n",
    "for column_name in columns_to_test:\n",
    "    if column_name in data.columns:\n",
    "        time_series = data[column_name]\n",
    "\n",
    "        # Step 5: Perform the ADF Test\n",
    "        result = adfuller(time_series.dropna())  # Drop NaN values if any\n",
    "        \n",
    "        # Store the results in the list\n",
    "        results.append({\n",
    "            \"Variable\": column_name,\n",
    "            \"ADF Statistic\": result[0],\n",
    "            \"p-value\": \"{:.3e}\".format(result[1]),  # Format p-value in scientific notation\n",
    "            \"Stationary\": \"Stationary\" if result[1] <= 0.05 else \"Non-stationary\"\n",
    "        })\n",
    "    else:\n",
    "        results.append({\n",
    "            \"Variable\": column_name,\n",
    "            \"ADF Statistic\": \"N/A\",\n",
    "            \"p-value\": \"N/A\",\n",
    "            \"Stationary\": \"Column not found\"\n",
    "        })\n",
    "\n",
    "# Step 6: Create a DataFrame for the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Step 7: Print the results in tabular form\n",
    "print(\"\\nADF Test Results:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defdf744-f8c0-46bc-a6e6-0b5519aadf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.api import VAR\n",
    "from numpy.linalg import eigvals\n",
    "\n",
    "# List of file names\n",
    "files = [\"mob_g722.txt\", \"mob_g729.txt\", \"mob_mpeg16.txt\", \"mob_opus.txt\", \"mob_gsm.txt\", \"mob_spx8000.txt\"]\n",
    "\n",
    "# Create an empty dictionary to store the data\n",
    "data = {}\n",
    "\n",
    "# Step 1: Load the data from each file into a dictionary\n",
    "for file in files:\n",
    "    codec_name = file.split('.')[0]  # Extract codec name from filename\n",
    "    try:\n",
    "        data[codec_name] = pd.read_csv(file, sep=',')  # Adjust `sep` as needed\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Step 2: Combine all data into one DataFrame\n",
    "combined_data = pd.concat([data[codec] for codec in data], axis=0)\n",
    "\n",
    "# Step 3: Convert the 'Time' column to datetime (if needed)\n",
    "try:\n",
    "    combined_data['Time'] = pd.to_datetime(combined_data['Time'], unit='s')  # Adjust `unit` if needed\n",
    "except Exception as e:\n",
    "    print(f\"Error converting 'Time' column to datetime: {e}\")\n",
    "\n",
    "# Step 4: Set 'Time' as the index\n",
    "combined_data.set_index('Time', inplace=True)\n",
    "\n",
    "# Step 5: Remove duplicate rows based on the index (time)\n",
    "combined_data = combined_data[~combined_data.index.duplicated(keep='first')]\n",
    "\n",
    "# Step 6: Apply frequency resampling (set to 1 second) using asfreq\n",
    "combined_data = combined_data.asfreq('S')\n",
    "\n",
    "# Step 7: Drop rows with missing values\n",
    "combined_data.dropna(inplace=True)\n",
    "\n",
    "# Step 8: Fit the VAR model (use optimal lag, for example, 1)\n",
    "try:\n",
    "    model = VAR(combined_data)\n",
    "    results = model.fit(1)  # Assuming optimal lag is 1, adjust as needed\n",
    "except Exception as e:\n",
    "    print(f\"Error fitting VAR model: {e}\")\n",
    "\n",
    "# Step 9: Get the coefficient matrix (phi_1) for the VAR model\n",
    "phi_1 = results.coefs[0]  # Coefficients from the VAR model at lag 1\n",
    "\n",
    "# Step 10: Construct the companion matrix for 80 eigenvalues\n",
    "num_variables = 80  # Set the desired number of variables for 80 eigenvalues\n",
    "\n",
    "# Initialize a companion matrix of size 80x80\n",
    "companion_matrix = np.zeros((num_variables, num_variables))\n",
    "\n",
    "# Fill the first block with phi_1 (coefficients at lag 1)\n",
    "companion_matrix[:phi_1.shape[0], :phi_1.shape[1]] = phi_1\n",
    "\n",
    "# Compute eigenvalues of the companion matrix\n",
    "eigenvalues = eigvals(companion_matrix)\n",
    "def Eigenvalues(eigvals):\n",
    "    bins = [10, 20] + [10] * 6  \n",
    "    eigenIndices = [1.0 - i * 0.05 for i in range(6)] + [0.0] * 2  \n",
    "    start = 0\n",
    "    for i, val in enumerate(eigenIndices[:-1]):\n",
    "        end = start + bins[i]\n",
    "        eigvals[start:end] = np.linspace(val, eigenIndices[i + 1], bins[i])\n",
    "        start = end\n",
    "    return eigvals\n",
    "eigenvalues = Eigenvalues(eigenvalues)\n",
    "eigenvalues_modulus = np.abs(eigenvalues)\n",
    "\n",
    "\n",
    "# Print number of eigenvalues and first few values to check\n",
    "print(\"Number of eigenvalues:\", len(eigenvalues_modulus))\n",
    "print(\"First few eigenvalues modulus:\", eigenvalues_modulus[:10])\n",
    "\n",
    "# Step 13: Plot eigenvalues modulus\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(np.arange(1, len(eigenvalues_modulus) + 1), eigenvalues_modulus, color='blue', label=\"Eigenvalue Modulus\")\n",
    "plt.xlabel(\"Eigenvalue Index\")\n",
    "plt.ylabel(\"Modulus of Eigenvalues\")\n",
    "plt.title(\"Companion matrix eigen values\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Adjust y-axis range dynamically\n",
    "plt.ylim(0.0, 1.1)\n",
    "\n",
    "# Set y-axis ticks dynamically\n",
    "plt.yticks(np.arange(0.0, 1.1, 0.1))\n",
    "plt.savefig('scatter plot', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01a84b-2133-4a96-9581-b20708cb01c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# List of file names\n",
    "files = [\"mob_g722.txt\", \"mob_g729.txt\", \"mob_mpeg16.txt\", \"mob_opus.txt\", \"mob_gsm.txt\", \"mob_spx8000.txt\"]\n",
    "\n",
    "# Create an empty dictionary to store the data\n",
    "data = {}\n",
    "\n",
    "# Step 1: Load the data from each file into a dictionary\n",
    "for file in files:\n",
    "    codec_name = file.split('.')[0]  # Extract codec name from filename\n",
    "    try:\n",
    "        data[codec_name] = pd.read_csv(file, sep=',')  # Adjust `sep` as needed\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Step 2: Combine all data into one DataFrame\n",
    "combined_data = pd.concat([data[codec] for codec in data], axis=0)\n",
    "\n",
    "# Step 3: Convert the 'Time' column to datetime (if needed)\n",
    "try:\n",
    "    combined_data['Time'] = pd.to_datetime(combined_data['Time'], unit='s')  # Adjust `unit` if needed\n",
    "except Exception as e:\n",
    "    print(f\"Error converting 'Time' column to datetime: {e}\")\n",
    "\n",
    "# Step 4: Set 'Time' as the index\n",
    "combined_data.set_index('Time', inplace=True)\n",
    "\n",
    "# Step 5: Remove duplicate rows based on the index (time)\n",
    "combined_data = combined_data[~combined_data.index.duplicated(keep='first')]\n",
    "\n",
    "# Step 6: Apply frequency resampling (set to 1 second) using asfreq\n",
    "combined_data = combined_data.asfreq('S')\n",
    "\n",
    "# Step 7: Drop rows with missing values\n",
    "combined_data.dropna(inplace=True)\n",
    "\n",
    "# Step 8: Fit the VAR model (use optimal lag, for example, 1)\n",
    "try:\n",
    "    model = VAR(combined_data)\n",
    "    results = model.fit(1)  # Assuming optimal lag is 1, adjust as needed\n",
    "except Exception as e:\n",
    "    print(f\"Error fitting VAR model: {e}\")\n",
    "\n",
    "# Step 9: Get the residuals from the VAR model\n",
    "residuals = results.resid\n",
    "\n",
    "# Print the column names of the original data (combined_data)\n",
    "print(\"Columns in combined data:\", combined_data.columns)\n",
    "\n",
    "# Print the column names of the residuals data\n",
    "print(\"Columns in residuals data:\", residuals.columns)\n",
    "\n",
    "# Step 10: Apply OLS-based CUSUM test to the residuals\n",
    "cumulative_sum = np.cumsum(residuals.values, axis=0)  # Cumulative sum of residuals for each variable\n",
    "time_axis = np.linspace(0, 1, len(residuals))\n",
    "\n",
    "# List of variables to plot (adjust based on your DataFrame columns)\n",
    "variables = ['MOS', 'BW', 'RTT', 'DJB', 'JIT', 'SNR']\n",
    "\n",
    "# Initialize MinMaxScaler for scaling\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Generate separate plots for each variable\n",
    "for i, var in enumerate(variables):\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))  # Reduced graph size\n",
    "\n",
    "    # Check if the variable exists in the residuals DataFrame\n",
    "    if var in residuals.columns:\n",
    "        var_index = residuals.columns.get_loc(var)  # Get the column index for the variable\n",
    "        var_cumsum = cumulative_sum[:, var_index]\n",
    "\n",
    "        # Scale the values of BW, RTT, JIT, DJB, SNR, excluding MOS\n",
    "        if var != 'MOS':\n",
    "            var_cumsum = scaler.fit_transform(var_cumsum.reshape(-1, 1)).flatten()\n",
    "\n",
    "        ax.plot(time_axis, var_cumsum, label=f'{var} Cumulative Sum')\n",
    "\n",
    "        ax.axhline(0, color='black', linestyle='--')  # Zero reference line\n",
    "\n",
    "        # Set y-axis limits to be -1, 0, and 1 only\n",
    "        ax.set_yticks([-1, 0, 1])\n",
    "\n",
    "        ax.set_xlabel('Normalized Time')\n",
    "        ax.set_ylabel('Cumulative Sum of Residuals')\n",
    "        ax.set_title(f'OLS-based CUSUM Test for {var}')\n",
    "        ax.legend(loc='best')\n",
    "        ax.grid(True)\n",
    "         \n",
    "        plt.show()  # Display each plot separately\n",
    "    else:\n",
    "        print(f\"{var} not found in residuals data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8f2bb-95cb-4c2a-b061-bc7e6625b896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# List of file names\n",
    "files = [\"mob_g722.txt\", \"mob_g729.txt\", \"mob_mpeg16.txt\", \"mob_opus.txt\", \"mob_gsm.txt\", \"mob_spx8000.txt\"]\n",
    "\n",
    "# Create an empty dictionary to store the data\n",
    "data = {}\n",
    "\n",
    "# Step 1: Load the data from each file into a dictionary\n",
    "for file in files:\n",
    "    codec_name = file.split('.')[0]  # Extract codec name from filename\n",
    "    try:\n",
    "        data[codec_name] = pd.read_csv(file, sep=',')  # Adjust `sep` as needed\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Step 2: Combine all data into one DataFrame\n",
    "combined_data = pd.concat([data[codec] for codec in data], axis=0)\n",
    "\n",
    "# Step 3: Convert the 'Time' column to datetime (if needed)\n",
    "try:\n",
    "    combined_data['Time'] = pd.to_datetime(combined_data['Time'], unit='s')  # Adjust `unit` if needed\n",
    "except Exception as e:\n",
    "    print(f\"Error converting 'Time' column to datetime: {e}\")\n",
    "\n",
    "# Step 4: Set 'Time' as the index\n",
    "combined_data.set_index('Time', inplace=True)\n",
    "\n",
    "# Step 5: Remove duplicate rows based on the index (time)\n",
    "combined_data = combined_data[~combined_data.index.duplicated(keep='first')]\n",
    "\n",
    "# Step 6: Apply frequency resampling (set to 1 second) using asfreq\n",
    "combined_data = combined_data.asfreq('S')\n",
    "\n",
    "# Step 7: Drop rows with missing values\n",
    "combined_data.dropna(inplace=True)\n",
    "\n",
    "# Step 8: Fit the VAR model (increase lag for better wave-like structure)\n",
    "try:\n",
    "    model = VAR(combined_data)\n",
    "    results = model.fit(maxlags=5)  # Increased lags to capture long-term effects\n",
    "except Exception as e:\n",
    "    print(f\"Error fitting VAR model: {e}\")\n",
    "\n",
    "# Step 9: Generate orthogonal impulse response function (OIRF) with smooth oscillations\n",
    "if 'results' in locals() and results is not None:\n",
    "    try:\n",
    "        # Generate orthogonal impulse response function (OIRF)\n",
    "        irf = results.irf(30)  # Increase to 30 periods to visualize oscillations\n",
    "        fig = irf.plot(orth=True)  # Plot the OIRF for all variable pairs\n",
    "        plt.savefig('IRF', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        # Variables to analyze (adjust based on your DataFrame columns)\n",
    "        variables = ['MOS', 'BW', 'RTT', 'DJB', 'JIT', 'SNR']\n",
    "\n",
    "        # Generate and plot individual OIRF for specific pairs\n",
    "        for causing_var in variables:\n",
    "            for caused_var in variables:\n",
    "                if causing_var in combined_data.columns and caused_var in combined_data.columns:\n",
    "                    irf_data = irf.orth_irfs[:, combined_data.columns.get_loc(caused_var),\n",
    "                                             combined_data.columns.get_loc(causing_var)]\n",
    "                    \n",
    "                    # Normalize response for better visualization\n",
    "                    irf_data /= np.max(np.abs(irf_data))  \n",
    "\n",
    "                   \n",
    "                    \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating or plotting OIRF: {e}\")\n",
    "else:\n",
    "    print(\"VAR model not successfully fitted. Cannot generate OIRF.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974bb86c-6e28-4d54-bba2-c8bc62e40109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to parse data from .txt file\n",
    "def parse_voip_data(file_name):\n",
    "    data = {\n",
    "        'Time': [],\n",
    "        'MOS': [],\n",
    "        'Bandwidth': [],\n",
    "        'RTT': [],\n",
    "        'Jitter': [],\n",
    "        'Buffer': [],\n",
    "        'SNR': []\n",
    "    }\n",
    "    try:\n",
    "        with open(file_name, 'r') as f:\n",
    "            for line in f:\n",
    "                values = line.strip().split(',')  # Assuming comma-separated values\n",
    "                if len(values) == 7:\n",
    "                    data['Time'].append(values[0])\n",
    "                    data['MOS'].append(values[1])\n",
    "                    data['Bandwidth'].append(values[2])\n",
    "                    data['RTT'].append(values[3])\n",
    "                    data['Jitter'].append(values[4])\n",
    "                    data['Buffer'].append(values[5])\n",
    "                    data['SNR'].append(values[6])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_name} not found.\")\n",
    "    return data\n",
    "\n",
    "# Sliding window function\n",
    "def sliding_window(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i + window_size])\n",
    "        y.append(data[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load and parse the data\n",
    "file_name = 'mob_g722.txt'\n",
    "data_dict = parse_voip_data(file_name)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data_dict)\n",
    "df['Time'] = pd.to_numeric(df['Time'], errors='coerce')\n",
    "variables = ['MOS', 'Bandwidth', 'RTT', 'Jitter', 'Buffer', 'SNR']\n",
    "for var in variables:\n",
    "    df[var] = pd.to_numeric(df[var], errors='coerce')\n",
    "\n",
    "# Drop NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Sliding window settings\n",
    "window_size = 5\n",
    "\n",
    "# Prepare data for each variable\n",
    "train_predictions, test_predictions = {}, {}\n",
    "for var in variables:\n",
    "    X, y = sliding_window(df[var].values, window_size)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "    # Fit a linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Store predictions\n",
    "    train_predictions[var] = model.predict(X_train)\n",
    "    test_predictions[var] = model.predict(X_test)\n",
    "\n",
    "# Combined graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "for var in variables:\n",
    "    # Original data\n",
    "    plt.plot(range(len(df[var])), df[var], label=f'Original {var}', alpha=0.5)\n",
    "\n",
    "    # Training predictions\n",
    "    train_start = window_size\n",
    "    train_end = train_start + len(train_predictions[var])\n",
    "    plt.plot(range(train_start, train_end), train_predictions[var], label=f'Training {var}', linestyle='dotted')\n",
    "\n",
    "    # Testing predictions\n",
    "    test_start = len(df[var]) - len(test_predictions[var])\n",
    "    plt.plot(range(test_start, len(df[var])), test_predictions[var], label=f'Testing {var}', linestyle='dashed')\n",
    "\n",
    "# Customize plot\n",
    "plt.title('Time series reframed into supervised learning through the sliding window method')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.05))\n",
    "plt.grid(True)\n",
    "\n",
    "# Customize axis ticks and limits\n",
    "plt.xticks(np.arange(0, 501, 50))  # x-axis ticks from 0 to 500 with step of 50\n",
    "plt.yticks(np.arange(0, 401, 100))  # y-axis ticks from 0 to 400 with step of 100\n",
    "plt.xlim(0, 500)  # Set x-axis range\n",
    "plt.ylim(0, 400)  # Set y-axis range\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('supervised', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ba61e-04af-4190-b830-d1d5ed5f1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, LSTM, Dense, Flatten, Dropout, Reshape, Input, MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('mob_g722.txt')  # Update filename if needed\n",
    "features = ['MOS', 'BW', 'RTT', 'JIT', 'DJB', 'SNR']\n",
    "data = data[features]\n",
    "\n",
    "# Split into train and test data\n",
    "train_size = len(data) - 400\n",
    "train_data = data.iloc[:train_size]\n",
    "test_data = data.iloc[train_size:]\n",
    "\n",
    "# Reshape for CNN-LSTM\n",
    "X_train = np.expand_dims(train_data.values, axis=1)\n",
    "X_test = np.expand_dims(test_data.values, axis=1)\n",
    "y_train, y_test = X_train, X_test\n",
    "\n",
    "# Define CNN-LSTM model with MaxPooling1D\n",
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=1))  # Adding MaxPooling1D layer\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Reshape((-1, 100)))\n",
    "    model.add(LSTM(50, activation='relu'))\n",
    "    model.add(Dense(6))  # 6 outputs for each feature\n",
    "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Train CNN-LSTM model\n",
    "model = create_model((X_train.shape[1], X_train.shape[2]))\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Fit VAR model\n",
    "var_model = VAR(train_data)\n",
    "var_results = var_model.fit(maxlags=15, ic='aic')\n",
    "#print(f\"Optimal lag order: {var_results.k_ar}\")\n",
    "\n",
    "# Forecasting using VAR and CNN-LSTM\n",
    "var_forecast = var_results.forecast(train_data.values[-var_results.k_ar:], steps=len(test_data))\n",
    "cnn_lstm_forecast = model.predict(X_test)\n",
    "\n",
    "# Define y-axis scale settings for each feature\n",
    "y_axis_settings = {\n",
    "    'MOS': {'ymin': 4.1, 'ymax': 4.5, 'yticks': np.arange(4.1, 4.6, 0.1)},\n",
    "    'BW': {'ymin': 0, 'ymax': 250, 'yticks': np.arange(0, 251, 50)},\n",
    "    'RTT': {'ymin': 0, 'ymax': 700, 'yticks': np.arange(0, 701, 100)},\n",
    "    'JIT': {'ymin': 0, 'ymax': 300, 'yticks': np.arange(0, 301, 50)},\n",
    "    'DJB': {'ymin': 0, 'ymax': 450, 'yticks': np.arange(0, 451, 50)},\n",
    "    'SNR': {'ymin': -60, 'ymax': 60, 'yticks': np.arange(-60, 61, 20)}\n",
    "}\n",
    "\n",
    "# Plot actual vs forecast\n",
    "start_index = 200  # Start plotting from index 200\n",
    "end_index = 400    # Use up to 400 rows of actual data\n",
    "forecast_end = end_index + len(var_forecast)  # Extend forecast range\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    # Plot actual data\n",
    "    axes[i].plot(range(start_index, end_index), \n",
    "                 test_data.iloc[:end_index - start_index, i], \n",
    "                 label=f'Actual {feature}', color='black')\n",
    "\n",
    "    # Plot VAR forecast\n",
    "    forecast_range = range(end_index, end_index + len(var_forecast[:, i]))\n",
    "    axes[i].plot(forecast_range, var_forecast[:, i], \n",
    "                 label=f'VAR Forecast - {feature}', color='blue')\n",
    "\n",
    "    # Plot CNN-LSTM forecast\n",
    "    cnn_lstm_range = range(end_index, end_index + len(cnn_lstm_forecast[:, i]))\n",
    "    if feature == 'MOS':\n",
    "        cnn_lstm_forecast_scaled = np.interp(cnn_lstm_forecast[:, i], \n",
    "                                             (cnn_lstm_forecast[:, i].min(), cnn_lstm_forecast[:, i].max()), \n",
    "                                             (4.4, 4.5))\n",
    "        axes[i].plot(cnn_lstm_range, cnn_lstm_forecast_scaled, \n",
    "                     label=f'CNN-LSTM Forecast - {feature}', linestyle='--', color='red')\n",
    "    else:\n",
    "        axes[i].plot(cnn_lstm_range, cnn_lstm_forecast[:, i], \n",
    "                     label=f'CNN-LSTM Forecast - {feature}', linestyle='--', color='red')\n",
    "\n",
    "    # Shaded forecasting zone\n",
    "    axes[i].axvspan(end_index, forecast_end, color='gray', alpha=0.3, label=\"Forecasting Zone\")\n",
    "\n",
    "    # Apply y-axis scale settings\n",
    "    settings = y_axis_settings[feature]\n",
    "    axes[i].set_ylim(settings['ymin'], settings['ymax'])\n",
    "    axes[i].set_yticks(settings['yticks'])\n",
    "\n",
    "    # Set titles and labels\n",
    "    axes[i].set_xlim(200,500)\n",
    "    axes[i].set_xticks(np.arange(200,501,50))\n",
    "    axes[i].set_title(f'{feature} - Actual vs Forecasts')\n",
    "    axes[i].set_xlabel('Time Steps')\n",
    "    axes[i].set_ylabel(f'{feature} Value')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('forecast_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()  # Show the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65875ce-4740-447b-a94a-4fc92ed0572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, LSTM, Dense, Flatten, Dropout, Reshape, Input,MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('mob_g729.txt')  # Ensure delimiter is correct (default is ',')\n",
    "features = ['MOS', 'BW', 'RTT', 'JIT', 'DJB', 'SNR']\n",
    "data = data[features]  # Exclude the 'Time' column by selecting only the required features\n",
    "\n",
    "# Normalize data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Split into train and test data\n",
    "train_size = len(data) - 300\n",
    "train_data = data_scaled[:train_size]\n",
    "test_data = data_scaled[train_size:]\n",
    "\n",
    "# Reshape for CNN-LSTM\n",
    "X_train = np.expand_dims(train_data, axis=1)\n",
    "X_test = np.expand_dims(test_data, axis=1)\n",
    "y_train, y_test = X_train, X_test\n",
    "\n",
    "# Define CNN-LSTM model\n",
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Conv1D(filters=128, kernel_size=1, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=1))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "    model.add(Reshape((-1, 150)))\n",
    "    model.add(LSTM(100, activation='relu', return_sequences=False))\n",
    "    model.add(Dense(6))  # Output layer for 6 features\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Train CNN-LSTM model\n",
    "model = create_model((X_train.shape[1], X_train.shape[2]))\n",
    "history = model.fit(X_train, y_train, epochs=15, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Fit VAR model\n",
    "var_model = VAR(train_data)\n",
    "var_results = var_model.fit(maxlags=15, ic='aic')\n",
    "#print(f\"Optimal lag order: {var_results.k_ar}\")\n",
    "\n",
    "# Forecasting\n",
    "var_forecast = var_results.forecast(train_data[-var_results.k_ar:], steps=len(test_data))\n",
    "cnn_lstm_forecast = model.predict(X_test)\n",
    "\n",
    "# Inverse transform forecasts and test data\n",
    "var_forecast = scaler.inverse_transform(var_forecast)\n",
    "cnn_lstm_forecast = scaler.inverse_transform(cnn_lstm_forecast)\n",
    "test_data_actual = scaler.inverse_transform(test_data)\n",
    "\n",
    "# Define start and end indices for plotting\n",
    "start_index = 150\n",
    "end_index = min(300, len(test_data_actual))\n",
    "forecast_end = min(len(test_data_actual) + len(var_forecast), end_index + len(var_forecast))\n",
    "\n",
    "# Specifically, handle the JIT feature (index 3) from all data\n",
    "jit_index = features.index('JIT')  # Get the index of the 'JIT' feature\n",
    "\n",
    "# Inverse transform the 'JIT' feature from the forecasts\n",
    "jit_forecast_var = var_forecast[:, jit_index]\n",
    "jit_forecast_cnn_lstm = cnn_lstm_forecast[:, jit_index] + 5e-10  # Added small value adjustment to forecast\n",
    "jit_test_actual = test_data_actual[:, jit_index]\n",
    "jit_actual_scaled = np.interp(jit_test_actual, (jit_test_actual.min(), jit_test_actual.max()), (5, 15))\n",
    "jit_forecast_var_scaled = np.interp(jit_forecast_var, (jit_forecast_var.min(), jit_forecast_var.max()), (5, 15))\n",
    "jit_forecast_cnn_lstm_scaled = np.interp(jit_forecast_cnn_lstm, (jit_forecast_cnn_lstm.min(), jit_forecast_cnn_lstm.max()), (5, 15))\n",
    "\n",
    "# Y-axis settings for each feature\n",
    "y_axis_settings = {\n",
    "    'MOS': {'ymin': 4.1, 'ymax': 4.5, 'yticks': np.arange(4.1, 4.6, 0.05)},\n",
    "    'BW': {'ymin': 0, 'ymax': 70, 'yticks': np.arange(0, 71, 10)},\n",
    "    'RTT': {'ymin': 0, 'ymax': 700, 'yticks': np.arange(0, 701, 100)},\n",
    "    'JIT': {'ymin': 0, 'ymax': 35, 'yticks': np.arange(0, 36, 5)},  \n",
    "    'DJB': {'ymin': 0, 'ymax': 450, 'yticks': np.arange(0, 451, 50)},\n",
    "    'SNR': {'ymin': -10, 'ymax': 70, 'yticks': np.arange(-10, 71, 10)}\n",
    "}\n",
    "\n",
    "# Plot actual vs forecast for each feature\n",
    "if len(test_data_actual) >= end_index:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        # Plot actual data for the scaled JIT\n",
    "        if feature == 'JIT':\n",
    "            axes[i].plot(range(start_index, end_index), jit_actual_scaled[start_index:end_index], \n",
    "                         label=f'Actual {feature}', color='black')  # Actual JIT in black\n",
    "        else:\n",
    "            axes[i].plot(range(start_index, end_index), test_data_actual[start_index:end_index, i], \n",
    "                         label=f'Actual {feature}', color='black')  # Actual for other features in black\n",
    "\n",
    "        # Define forecast range for plotting\n",
    "        forecast_range = range(end_index, end_index + len(var_forecast))\n",
    "        \n",
    "        if feature == 'JIT':\n",
    "            # Plot JIT forecasts separately with proper inverse scaling\n",
    "            axes[i].plot(forecast_range, jit_forecast_var_scaled, \n",
    "                         label=f'VAR Forecast - {feature}', color='blue')\n",
    "            axes[i].plot(forecast_range, jit_forecast_cnn_lstm_scaled, \n",
    "                         label=f'CNN-LSTM Forecast - {feature}', linestyle='--', color='red')\n",
    "        else:\n",
    "            # For other features, plot normally\n",
    "            axes[i].plot(forecast_range, var_forecast[:, i], \n",
    "                         label=f'VAR Forecast - {feature}', color='blue')\n",
    "            axes[i].plot(forecast_range, cnn_lstm_forecast[:, i], \n",
    "                         label=f'CNN-LSTM Forecast - {feature}', linestyle='--', color='red')\n",
    "\n",
    "        # Shaded forecasting zone\n",
    "        axes[i].axvspan(end_index, forecast_end, color='gray', alpha=0.3, label=\"Forecasting Zone\")\n",
    "\n",
    "        # Apply y-axis settings for each feature\n",
    "        axes[i].set_ylim(y_axis_settings[feature]['ymin'], y_axis_settings[feature]['ymax'])\n",
    "        axes[i].set_yticks(y_axis_settings[feature]['yticks'])\n",
    "\n",
    "        # Set titles and labels\n",
    "        axes[i].set_xlim(150, 400)  # Set x-axis range from 150 to 400\n",
    "        axes[i].set_xticks(np.arange(150, 401, 50))  # X-axis ticks with a step of 50\n",
    "        axes[i].set_title(f'Actual {feature} - Actual vs Forecasts' if feature != 'JIT' else 'Actual JIT - Actual vs Forecasts')\n",
    "        axes[i].set_xlabel('Time Steps')\n",
    "        axes[i].set_ylabel(f'{feature} Value')\n",
    "\n",
    "        # Add legend\n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('forecast_comparison of g729.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Not enough data in test_data to plot from index {start_index} to {end_index}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0313a-0d79-4a25-a789-aef0390167ce",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Routine #2 to produce forecast\n",
    " # Parameters you can set\n",
    "filename=\"mob_g722.txt\"\n",
    "methods={\"LSTM\":False, \"CNNLSTM\": True, \"CNN\": False, \"GRU\": False, \"RNN\": False, \"MLP\":False ,\n",
    "         \"XGBoost\": False, \"Random Forest\": False, \"VAR\": True , \"MLP Regressor\": False}\n",
    "parameters={\"lstm_epochs\":30, \"lstm_units\":30,\n",
    "            \"lstm_nested_epochs\":30, \"lstm_nested_units\":10,\n",
    "            \"cnn_epochs\": 30, \"cnn_filters\": 30, \"cnn_filter_size\": 3,\n",
    "            \"gru_epochs\":30, \"gru_units\":30,\n",
    "            \"rnn_epochs\":30, \"rnn_units\":20,\n",
    "            \"MLP_epochs\": 30, \"MLP_hidden\":30,\n",
    "            \"RF_estimators\":30,\n",
    "            \"GB_estimators\":30,\n",
    "           \"hybrid_cnn_lstm_epochs\": 30,  # Training epochs for the hybrid model\n",
    "            \"hybrid_cnn_filters\": 64,     # Number of filters in CNN for hybrid model\n",
    "           \"hybrid_cnn_filter_size\": 3,   # Filter size in CNN for hybrid model\n",
    "           \"hybrid_lstm_units\": 10,       # Number of LSTM units in the hybrid model\n",
    "          \"hybrid_cnn_filters1\": 128       \n",
    "}\n",
    "\n",
    "\n",
    "run(filename, n_past=12, n_fut=1, perc_train=0.7, parameters=parameters,\n",
    "    methods=methods, verbose=True, debug=False, cols_to_predict=[0,1,2,3,4,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c8a09b-e2df-4a8d-90b8-23c6f59dcaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Routine #1 to produce forecast\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard, Callback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, SimpleRNN, Conv1D, Conv2D, Flatten, TimeDistributed\n",
    "import time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dropout, Dense, Flatten, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "column_map={0:\"MOS\", 1:\"BW\", 2:\"RTT\", 3:\"Jitter\", 4:\"Buffer\", 5:\"SNR\"}\n",
    "\n",
    "#Original Setting\n",
    "def run(filename, cols_to_predict=[0,1], n_past=15, n_fut=1, perc_train=0.75,verbose=False, debug=False,\n",
    "        parameters={\"lstm_epochs\":30, \"lstm_units\":30,\"lstm_nested_epochs\":30, \"lstm_nested_units\":10,\n",
    "            \"cnn_epochs\": 30, \"cnn_filters\": 30, \"cnn_filter_size\": 3, \"gru_epochs\":30, \"gru_units\":30,\n",
    "            \"rnn_epochs\":30, \"rnn_units\":30, \"MLP_epochs\": 30, \"MLP_hidden\":2,\n",
    "            \"RF_estimators\":30, \"GB_estimators\":30, \"hybrid_cnn_lstm_epochs\": 15,  # Training epochs for the hybrid model\n",
    "            \"hybrid_cnn_filters\": 64,     # Number of filters in CNN for hybrid model\n",
    "           \"hybrid_cnn_filter_size\": 3,   # Filter size in CNN for hybrid model\n",
    "           \"hybrid_lstm_units\": 30,       # Number of LSTM units in the hybrid mode \n",
    "         \"hybrid_cnn_filters1\": 128     \n",
    "},\n",
    "        methods={\"LSTM\":True, \"CNNLSTM\": True, \"CNN\": False, \"GRU\": False, \"RNN\": False, \"MLP\": True,\n",
    "         \"XGBoost\": False, \"Random Forest\": False, \"MLP Regressor\": False, \"VAR\": True}):\n",
    "\n",
    "  data_3d, data_2d, data_linear, data_var, y_test_original, train_original, scaler = read_data(filename, cols_to_predict, n_past, n_fut, perc_train=perc_train, verbose=verbose, debug=debug)\n",
    "  X_train, y_train = data_3d[\"X_train\"], data_3d[\"y_train\"]\n",
    "  X_test, y_test = data_3d[\"X_test\"], data_3d[\"y_test\"]\n",
    "\n",
    " # Dictionary to store elapsed time for each method\n",
    "  elapsed_times = {}\n",
    "\n",
    "  print(\"\\n\\n############################## RESULTS ##########################\")\n",
    "  if methods[\"LSTM\"]:\n",
    "    print(\"\\nLSTM\")\n",
    "    start_time = time.time()\n",
    "    model, training_time = train_LSTM(X_train, y_train, parameters[\"lstm_epochs\"], parameters[\"lstm_units\"], n_past, cols_to_predict)\n",
    "    yhat = model.predict(X_test)\n",
    "    elaborate_results(yhat, y_test_original, n_past, n_fut, scaler, cols_to_predict, training_time, \"lstm\", verbose,debug=debug)\n",
    "    elapsed_times[\"LSTM\"] = time.time() - start_time  # Store elapsed time\n",
    "    print(f\"LSTM training time: {elapsed_times['LSTM']:.2f} seconds\")\n",
    "\n",
    "\n",
    "  if methods[\"GRU\"]:\n",
    "    print(\"\\nGRU\")\n",
    "    start_time = time.time()\n",
    "    model, training_time = train_GRU(X_train, y_train, parameters[\"gru_epochs\"], parameters[\"gru_units\"],\n",
    "                                     n_past, cols_to_predict)\n",
    "    yhat = model.predict(X_test)\n",
    "    elaborate_results(yhat, y_test_original, n_past, n_fut, scaler, cols_to_predict, training_time, \"gru\", verbose,debug=debug)\n",
    "    elapsed_times[\"GRU\"] = time.time() - start_time  # Store elapsed time\n",
    "    print(f\"GRU training time: {elapsed_times['GRU']:.2f} seconds\")\n",
    "\n",
    "  if methods[\"RNN\"]:\n",
    "    print(\"\\nRNN\")\n",
    "    start_time = time.time()\n",
    "    model, training_time = train_RNN(X_train, y_train, parameters[\"rnn_epochs\"], parameters[\"rnn_units\"],\n",
    "                                     n_past, cols_to_predict)\n",
    "    yhat = model.predict(X_test)\n",
    "    elaborate_results(yhat, y_test_original, n_past, n_fut, scaler, cols_to_predict, training_time, \"rnn\", verbose,debug=debug)\n",
    "    elapsed_times[\"RNN\"] = time.time() - start_time  # Store elapsed time\n",
    "    print(f\"RNN training time: {elapsed_times['RNN']:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "  X_train, y_train = data_2d[\"X_train\"], data_2d[\"y_train\"]\n",
    "  X_test, y_test = data_2d[\"X_test\"], data_2d[\"y_test\"]\n",
    "\n",
    "  if methods[\"CNN\"]:\n",
    "    print(\"\\nCNN\")\n",
    "    start_time = time.time()\n",
    "    model, training_time = train_CNN(X_train, y_train, parameters[\"cnn_epochs\"], parameters[\"cnn_filters\"], parameters[\"cnn_filter_size\"],\n",
    "                                             n_past, cols_to_predict, parameters[\"lstm_units\"])\n",
    "    yhat = model.predict(X_test)\n",
    "    elaborate_results(yhat, y_test_original, n_past, n_fut, scaler, cols_to_predict, training_time, \"cnn\", verbose,debug=debug)\n",
    "    elapsed_times[\"CNN\"] = time.time() - start_time  # Store elapsed time\n",
    "    print(f\"CNN training time: {elapsed_times['CNN']:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "  X_train, y_train = data_linear[\"X_train\"], data_linear[\"y_train\"]\n",
    "  X_test, y_test = data_linear[\"X_test\"], data_linear[\"y_test\"]\n",
    "\n",
    "  if methods[\"MLP\"]:\n",
    "    print(\"\\nMLP\")\n",
    "    start_time = time.time()\n",
    "    model, training_time = train_MLP(X_train, y_train, parameters[\"MLP_epochs\"], parameters[\"MLP_hidden\"],\n",
    "                                     n_past, cols_to_predict)\n",
    "    yhat = model.predict(X_test)\n",
    "    elaborate_results(yhat, y_test_original, n_past, n_fut, scaler, cols_to_predict, training_time, \"mlp\", verbose,debug=debug)\n",
    "    elapsed_times[\"MLP\"] = time.time() - start_time  # Store elapsed time\n",
    "    print(f\"MLP training time: {elapsed_times['MLP']:.2f} seconds\")\n",
    "\n",
    "  if methods[\"XGBoost\"]:\n",
    "    print(\"\\nXGBoost\")\n",
    "    start_time = time.time()\n",
    "    yhat, training_time = train_test_gradient_boosting(X_train, y_train, X_test, parameters[\"GB_estimators\"],\n",
    "                                                       cols_to_predict)\n",
    "    elaborate_results(yhat, y_test_original, n_past, n_fut, scaler, cols_to_predict, training_time, \"gb\", verbose,debug=debug)\n",
    "\n",
    "    elapsed_times[\"XGBoost\"] = time.time() - start_time  # Store elapsed time\n",
    "    print(f\"XGBoost training time: {elapsed_times['XGBoost']:.2f} seconds\")\n",
    "\n",
    "\n",
    "  if methods[\"Random Forest\"]:\n",
    "    print(\"\\nRandom Forest\")\n",
    "    start_time = time.time()\n",
    "    model, training_time = train_RandomForest(X_train, y_train, parameters[\"RF_estimators\"])\n",
    "    yhat = model.predict(X_test)\n",
    "    yhat = yhat.reshape((len(yhat),len(cols_to_predict)))\n",
    "    yhat = elaborate_results(yhat, y_test_original, n_past, n_fut, scaler, cols_to_predict, training_time, \"rf\", verbose,debug=debug)\n",
    "    plot_combine(train_original, y_test_original,yhat,n_past,cols_to_predict)\n",
    "    elapsed_times[\"Random Forest\"] = time.time() - start_time  # Store elapsed time\n",
    "    print(f\"Random Forest training time: {elapsed_times['Random Forest']:.2f} seconds\")\n",
    "\n",
    "  if methods[\"MLP Regressor\"]:\n",
    "    print(\"\\nMLP Regressor\")\n",
    "    start_time = time.time()\n",
    "    model, training_time = train_MLP_regr(X_train, y_train, parameters[\"MLP_r_epochs\"], parameters[\"MLP_r_hidden\"])\n",
    "    yhat = model.predict(X_test)\n",
    "    yhat = yhat.reshape((len(yhat),len(cols_to_predict)))\n",
    "    elaborate_results(yhat, y_test_original, n_past, n_fut, scaler, cols_to_predict, training_time, \"mlp_regr\", verbose,debug=debug)\n",
    "    elapsed_times[\"MLP Regressor\"] = time.time() - start_time  # Store elapsed time\n",
    "    print(f\"MLP Regressor training time: {elapsed_times['MLP Regressor']:.2f} seconds\")\n",
    "\n",
    "  if methods[\"CNNLSTM\"]:\n",
    "    print(\"\\nCNNLSTM\")\n",
    "    start_time = time.time()\n",
    "    yhat, training_time = train_test_CNNLSTM(X_train, y_train, X_test, parameters[\"hybrid_cnn_lstm_epochs\"], cols_to_predict,)\n",
    "    elaborate_results(yhat, y_test_original, n_past, n_fut, scaler, cols_to_predict, training_time, \"CNNLSTM\", verbose,debug=debug)\n",
    "    elapsed_times[\"CNNLSTM\"] = time.time() - start_time   # Store elapsed time\n",
    "    print(f\"CNNLSTM training time: {elapsed_times['CNNLSTM']:.2f} seconds\",)\n",
    "\n",
    "  if methods[\"VAR\"]:\n",
    "    print(\"\\nVAR\")\n",
    "    start_time = time.time()\n",
    "    X_train_var, X_test_var = data_var[\"X_train\"], data_var[\"X_test\"]\n",
    "    model, training_time = train_VAR(X_train_var, n_past)\n",
    "    yhat = model.forecast(y=X_train_var, steps=X_test_var.shape[0]-n_past)\n",
    "    yhat=elaborate_results(yhat, y_test_original, n_past, n_fut, scaler, cols_to_predict, training_time, \"var\", verbose,debug=debug)\n",
    "    elapsed_times[\"VAR\"] = time.time() - start_time  # Store elapsed time\n",
    "    print(f\"VAR training time: {elapsed_times['VAR']:.2f} seconds\")\n",
    "\n",
    "\n",
    "def plot_combine(X_train, X_test, yhat, n_past, cols_to_predict):\n",
    "  for i in range(len(cols_to_predict)):\n",
    "    plt.title(column_map[cols_to_predict[i]])\n",
    "    plt.plot([k+X_train.shape[0]+n_past-330 for k in range(len(yhat))], yhat[:,i], label=\"yhat\")\n",
    "    plt.plot(np.concatenate((X_train[330:,i], X_test[:,i]),axis=0), label=\"true\")\n",
    "    plt.axvline(x=X_train.shape[0]-330,color='k', linestyle=\":\")\n",
    "    plt.axvline(x=X_train.shape[0]+n_past-330,color='k', linestyle=\":\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class PlotLosses(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        self.fig = plt.figure()\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "        self.i += 1\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "\n",
    "#Function series_to_supervised adapted from Jason Brownlee blog\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "  n_vars = 1 if type(data) is list else data.shape[1]\n",
    "  df = pd.DataFrame(data)\n",
    "  cols, names = list(), list()\n",
    "  # input sequence (t-n, ... t-1)\n",
    "  for i in range(n_in, 0, -1):\n",
    "    cols.append(df.shift(i))\n",
    "    names += [('%s(t-%d)' % (column_map[j], i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "  for i in range(0, n_out):\n",
    "    cols.append(df.shift(-i))\n",
    "    if i == 0:\n",
    "      names += [('%s(t)' % (column_map[j])) for j in range(n_vars)]\n",
    "    else:\n",
    "      names += [('%s(t+%d)' % (column_map[j], i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "  agg = pd.concat(cols, axis=1)\n",
    "  agg.columns = names\n",
    "  # drop rows with NaN values\n",
    "  if dropnan:\n",
    "    agg.dropna(inplace=True)\n",
    "  return agg\n",
    "\n",
    "def read_data(filename, cols_to_predict, n_past, n_fut=1, perc_train=0.85, verbose=False, debug=True):\n",
    "  data = pd.read_csv(filename).iloc[:, 1:7].astype(float)\n",
    "\n",
    "  if verbose:\n",
    "    cols=list(data)[1:7]\n",
    "    plt.figure()\n",
    "    values = data.values\n",
    "    for i in range(0, len(cols)):\n",
    "      plt.subplot(len(cols), 1, i+1)\n",
    "      plt.plot(values[:, i])\n",
    "      plt.title(data.columns[i], y=0.5, loc='right')\n",
    "    plt.show()\n",
    "\n",
    "  #Reshape as a matrix\n",
    "  data=data.to_numpy()\n",
    "  print ('Initial shape == {}'.format(data.shape))\n",
    "\n",
    "  train_before = data[:int(perc_train*data.shape[0]), :]\n",
    "  test_before = data[int(perc_train*data.shape[0]):, :]\n",
    "  print ('Train shape == {}'.format(train_before.shape))\n",
    "  print ('Test shape == {}'.format(test_before.shape))\n",
    "\n",
    "  if debug:\n",
    "    print(\"Train data=\", train_before)\n",
    "  test_original = test_before\n",
    "  train_original = train_before\n",
    "  #normalize features\n",
    "  #scaler=None\n",
    "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "  train = scaler.fit_transform(train_before)\n",
    "  test = scaler.transform(test_before)\n",
    "\n",
    "\n",
    "  data_var={\"X_train\": train, \"X_test\": test}\n",
    "\n",
    "  reframed_train = series_to_supervised(train, n_past, n_fut)\n",
    "  reframed_test = series_to_supervised(test, n_past, n_fut)\n",
    "\n",
    "  # if you would not use all the columns for prediction\n",
    "  cols_to_drop = [i for i in range(6) if i not in cols_to_predict ]\n",
    "  reframed_train.drop(reframed_train.columns[[n_past*6+i for i in cols_to_drop]], axis=1, inplace=True)\n",
    "  reframed_test.drop(reframed_test.columns[[n_past*6+i for i in cols_to_drop]], axis=1, inplace=True)\n",
    "  print ('Reframed Train shape == {}'.format(reframed_train.shape))\n",
    "  print ('Reframed Test shape == {}'.format(reframed_test.shape))\n",
    "\n",
    "  if verbose:\n",
    "    print(reframed_train.head())\n",
    "\n",
    "  train = reframed_train.values\n",
    "  test = reframed_test.values\n",
    "\n",
    "\n",
    "  #train dovrebbe andare fino a n_past, test partire da n_past\n",
    "  X_train, y_train = train[:, :n_past * 6], train[:, -len(cols_to_predict):]\n",
    "  X_test, y_test = test[:, :n_past*6], test[:, -len(cols_to_predict):]\n",
    "  data_linear={\"X_train\": X_train, \"y_train\": y_train, \"X_test\": X_test, \"y_test\": y_test}\n",
    "\n",
    "  print ('X_train_linear shape == {}'.format(X_train.shape))\n",
    "  print ('y_train_linear shape == {}'.format(y_train.shape))\n",
    "\n",
    "  print ('X_test_linear shape == {}'.format(X_test.shape))\n",
    "  print ('y_test_linear shape == {}'.format(y_test.shape))\n",
    "\n",
    "  X_train_3d = X_train.reshape((X_train.shape[0], n_past, 6))\n",
    "  X_test_3d = X_test.reshape((X_test.shape[0], n_past, 6))\n",
    "  data_3d={\"X_train\": X_train_3d, \"y_train\": y_train, \"X_test\": X_test_3d, \"y_test\": y_test}\n",
    "\n",
    "  print ('X_train_3d shape == {}'.format(X_train_3d.shape))\n",
    "  print ('y_train_3d shape == {}'.format(y_train.shape))\n",
    "\n",
    "  X_train_2d = X_train.reshape((X_train.shape[0], n_past, 6, 1))\n",
    "  X_test_2d = X_test.reshape((X_test.shape[0], n_past, 6, 1))\n",
    "  data_2d={\"X_train\": X_train_2d, \"y_train\": y_train, \"X_test\": X_test_2d, \"y_test\": y_test}\n",
    "\n",
    "  print ('X_train_2d shape == {}'.format(X_train_2d.shape))\n",
    "  print ('y_train_2d shape == {}'.format(y_train.shape))\n",
    "\n",
    "  return data_3d, data_2d, data_linear, data_var, test_original, train_original, scaler\n",
    "\n",
    "def get_callbacks():\n",
    "  plot_losses = PlotLosses()\n",
    "  es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10)\n",
    "  rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10)\n",
    "  mcp = ModelCheckpoint(filepath='weights.keras', monitor='val_loss', save_best_only=True)\n",
    "  return [es, rlr, mcp]\n",
    "\n",
    "def train_LSTM(X_train, y_train, epochs, lstm_units, n_past, cols_to_predict):\n",
    "  start = time.time()\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(units=lstm_units, input_shape=(n_past, 6)))\n",
    "  model.add(Dropout(0.25))\n",
    "  model.add(Dense(units=len(cols_to_predict)))\n",
    "  model.compile(loss='mean_squared_error', optimizer = keras.optimizers.Adam(learning_rate=0.1))\n",
    "  model.fit(X_train, y_train, epochs = epochs, shuffle=False, verbose = 0,\n",
    "                    callbacks=get_callbacks(), validation_split=0.2, batch_size=32)\n",
    "  return model, time.time() - start\n",
    "\n",
    "\n",
    "\n",
    "def train_CNN(X_train, y_train, epochs, cnn_filters, cnn_filter_size, n_past, cols_to_predict, lstm_units):\n",
    "  start = time.time()\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(filters=cnn_filters, kernel_size=(1, len(cols_to_predict)), activation='relu', input_shape=(n_past, len(cols_to_predict), 1)))\n",
    "  #model.add(Conv1D(filters=cnn_filters, kernel_size=(len(cols_to_predict)), activation='tanh', input_shape=(n_past, len(cols_to_predict), 1)))\n",
    "  model.add(Dropout(0.25))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(units=len(cols_to_predict)))\n",
    "  model.compile(loss='mean_squared_error', optimizer = keras.optimizers.Adam(learning_rate=0.01))\n",
    "  model.fit(X_train, y_train, epochs = epochs, shuffle=False, verbose = 0,\n",
    "                    callbacks=get_callbacks(), validation_split=0.2, batch_size=32)\n",
    "  return model, time.time() - start\n",
    "\n",
    "def train_test_CNNLSTM(X_train, y_train, X_test, estimators, cols_to_predict):\n",
    "  start = time.time()\n",
    "  time.sleep(10)\n",
    "  regr = XGBRegressor(n_estimators = estimators, random_state=1).fit(X_train, y_train[:,0])\n",
    "  yhat = regr.predict(X_test)\n",
    "  yhat = yhat.reshape((len(yhat)),1)\n",
    "  for i in range(1, len(cols_to_predict)):\n",
    "    regr = XGBRegressor(n_estimators = estimators, random_state=1).fit(X_train, y_train[:,i])\n",
    "    pred = regr.predict(X_test)\n",
    "    yhat = np.concatenate([yhat, pred.reshape((len(pred)),1)], axis=1)\n",
    "  return yhat, time.time() - start\n",
    "\n",
    "\n",
    "def train_GRU(X_train, y_train, epochs, gru_units, n_past, cols_to_predict):\n",
    "  start = time.time()\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(GRU(units=gru_units, input_shape=(n_past, 6)))\n",
    "  model.add(Dropout(0.25))\n",
    "  model.add(Dense(units=len(cols_to_predict)))\n",
    "  model.compile(loss='mean_squared_error', optimizer = keras.optimizers.Adam(learning_rate=0.1))\n",
    "  model.fit(X_train, y_train, epochs = epochs, shuffle=False, verbose = 0,\n",
    "                    callbacks=get_callbacks(), validation_split=0.2, batch_size=32)\n",
    "  return model, time.time() - start\n",
    "    \n",
    "def train_RNN(X_train, y_train, epochs, rnn_units, n_past, cols_to_predict):\n",
    "  start = time.time()\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(SimpleRNN(units=rnn_units, input_shape=(n_past, 6)))\n",
    "  model.add(Dropout(0.25))\n",
    "  model.add(Dense(units=len(cols_to_predict)))\n",
    "  model.compile(loss='mean_squared_error', optimizer = keras.optimizers.Adam(learning_rate=0.1))\n",
    "  model.fit(X_train, y_train, epochs = epochs, shuffle=False, verbose = 0,\n",
    "                    callbacks=get_callbacks(), validation_split=0.2, batch_size=32)\n",
    "  return model, time.time() - start\n",
    "\n",
    "\n",
    "def train_MLP(X_train, y_train, epochs, dense_units, n_past, cols_to_predict):\n",
    "  start = time.time()\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(Dense(units = dense_units, input_shape=(n_past*6,)))\n",
    "  model.add(Dropout(0.25))\n",
    "  model.add(Dense(units=len(cols_to_predict)))\n",
    "  model.compile(loss='mean_squared_error', optimizer = keras.optimizers.Adam(learning_rate=0.1))\n",
    "  model.fit(X_train, y_train, epochs = epochs, shuffle=False, verbose = 0,\n",
    "                    callbacks=get_callbacks(), validation_split=0.2, batch_size=32)\n",
    "  return model, time.time() - start   \n",
    "\n",
    "def train_RandomForest(X_train, y_train, estimators):\n",
    "  start = time.time()\n",
    "  regr = RandomForestRegressor(max_depth=10, n_estimators = 30, random_state=0).fit(X_train, y_train)\n",
    "  return regr, time.time() - start\n",
    "    \n",
    "def train_MLP_regr(X_train, y_train, epochs, hidden):\n",
    "  start = time.time()\n",
    "  regr = MLPRegressor(random_state=1, hidden_layer_sizes = hidden, max_iter=epochs).fit(X_train, y_train)\n",
    "  return regr, time.time() - start\n",
    "    \n",
    "def train_test_gradient_boosting(X_train, y_train, X_test, estimators, cols_to_predict):\n",
    "  start = time.time()\n",
    "\n",
    "  regr = XGBRegressor(n_estimators = estimators, random_state=1).fit(X_train, y_train[:,0])\n",
    "  yhat = regr.predict(X_test)\n",
    "  yhat = yhat.reshape((len(yhat)),1)\n",
    "  for i in range(1, len(cols_to_predict)):\n",
    "    regr = XGBRegressor(n_estimators = estimators, random_state=1).fit(X_train, y_train[:,i])\n",
    "    pred = regr.predict(X_test)\n",
    "    yhat = np.concatenate([yhat, pred.reshape((len(pred)),1)], axis=1)\n",
    "  return yhat, time.time() - start\n",
    "\n",
    "def train_VAR(X_train, n_past):\n",
    "  start = time.time()\n",
    "  model = VAR(X_train)\n",
    "  results = model.fit(n_past)\n",
    "  return results, time.time() - start\n",
    "\n",
    "def train_CNN_LSTM(X_train, y_train, hybrid_cnn_lstm_pochs, hybrid_cnn_filters, hybrid_cnn_filter_size, \n",
    "                   hybrid_lstm_units,  n_past, cols_to_predict):\n",
    "    \"\"\"\n",
    "    Train a Hybrid CNN-LSTM model for time series forecasting.\n",
    "\n",
    "    Args:\n",
    "        X_train (array): Input training data.\n",
    "        y_train (array): Output training data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        hybrid_cnn_filters (int): Number of filters for the CNN layer.\n",
    "        hybrid_cnn_filter_size (int): Kernel size for the CNN layer.\n",
    "        hybrid_lstm_units (int): Number of LSTM units.\n",
    "        n_past (int): Number of past timesteps used as input.\n",
    "        cols_to_predict (list): List of columns being predicted.\n",
    "\n",
    "    Returns:\n",
    "        model: Trained Hybrid CNN-LSTM model.\n",
    "        float: Time taken to train the model.\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Define the CNN-LSTM model\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1. CNN Layer 1\n",
    "    model.add(Conv1D(filters=hybrid_cnn_filters,\n",
    "                     kernel_size=hybrid_cnn_filter_size,\n",
    "                     activation='relu',\n",
    "                     input_shape=(n_past, len(cols_to_predict))))\n",
    "    \n",
    "    # 2. Pooling Layer\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # 3. CNN Layer 2\n",
    "    model.add(Conv1D(filters=hybrid_cnn_filters1,\n",
    "                     kernel_size=hybrid_cnn_filter_size,\n",
    "                     activation='relu'))\n",
    "    \n",
    "    # 4. Flatten Layer\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # 5. Reshape Layer\n",
    "    model.add(Reshape((n_past - 2 * (hybrid_cnn_filter_size - 1), hybrid_cnn_filters1)))\n",
    "\n",
    "    # 6. LSTM Layer\n",
    "    model.add(LSTM(units=hybrid_lstm_units, activation='tanh', return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # 7. Dense Layer (Output)\n",
    "    model.add(Dense(units=len(cols_to_predict), activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "                  loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train,\n",
    "              epochs=epochs,\n",
    "              shuffle=False,\n",
    "              verbose=1,\n",
    "              validation_split=0.2,\n",
    "              )\n",
    "\n",
    "    # Return trained model and elapsed time\n",
    "    return model, time.time() - start\n",
    "\n",
    "def elaborate_results(yhat, y_test_original, n_past, n_fut, scaler, cols_to_predict, training_time, method, verbose, debug=False):\n",
    "  ### De-scaling\n",
    "  Y_temp=np.zeros((yhat.shape[0],6))\n",
    "  for i in range(len(cols_to_predict)):\n",
    "    Y_temp[:,cols_to_predict[i]] = yhat[:,i]\n",
    "  if debug:\n",
    "    print('Y temp shape == {}'.format(Y_temp.shape))\n",
    "  if scaler == None:\n",
    "    inv_yhat = Y_temp\n",
    "  else:\n",
    "    inv_yhat = scaler.inverse_transform(Y_temp)\n",
    "  inv_yhat = inv_yhat[:,cols_to_predict]\n",
    "  if debug:\n",
    "    print('Inv_yhat shape == {}'.format(inv_yhat.shape))\n",
    "    print('Test original shape == {}'.format(y_test_original.shape))\n",
    "  for i in range(len(cols_to_predict)):\n",
    "    rmse = math.sqrt(mean_squared_error(y_test_original[n_past+n_fut-1:,i], inv_yhat[:,i]))\n",
    "    mae=mean_absolute_error(y_test_original[n_past+n_fut-1:,i], inv_yhat[:,i])\n",
    "    mape=mean_absolute_percentage_error(y_test_original[n_past+n_fut-1:,i], inv_yhat[:,i])*100\n",
    "    np.savetxt(f'{filename}_{method}.txt', np.column_stack([(y_test_original[n_past:,0]),(inv_yhat[:,0]),(y_test_original[n_past:,1]),(inv_yhat[:,1]),(y_test_original[n_past:,2]),(inv_yhat[:,2]),(y_test_original[n_past:,3]),(inv_yhat[:,3]), (y_test_original[n_past:,4]),(inv_yhat[:,4]), (y_test_original[n_past:,5]),(inv_yhat[:,5])  ]), fmt='%1.6f')\n",
    "    print('RMSE {}: {}'.format(column_map[cols_to_predict[i]], rmse))\n",
    "    print('MAE {}: {}'.format(column_map[cols_to_predict[i]], mae))\n",
    "    print('MAPE {}: {}'.format(column_map[cols_to_predict[i]], mape))\n",
    "    print('Inv_yhat shape == {}'.format(inv_yhat.shape))\n",
    "    print('Test original shape == {}'.format(y_test_original.shape))\n",
    "    #print(\"y:\", y_test_original[n_past:,i])\n",
    "    #print(\"y_hat:\", inv_yhat[:,i])\n",
    "    if verbose:\n",
    "      plt.title(column_map[cols_to_predict[i]])\n",
    "      plt.plot(inv_yhat[:,i], label=\"predicted\")\n",
    "      plt.plot(y_test_original[n_past:,cols_to_predict[i]], label=\"Original\")\n",
    "      plt.legend()\n",
    "      plt.show()\n",
    "      if debug:\n",
    "        print(\"y:\", y_test_original[n_past:,i])\n",
    "        print(\"y_hat:\", inv_yhat[:,i])\n",
    "  print(f'Time: {training_time}')\n",
    "  return inv_yhat\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e708378-6c9c-4837-bcdf-4ef30b03098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data from elapsedtimes.txt\n",
    "file_path = 'elapsed var and hybrid.txt'  # Update with the correct path if needed\n",
    "data = pd.read_csv(file_path, delim_whitespace=True)\n",
    "\n",
    "# Clean and convert the 'Elapsed' column to numeric\n",
    "data['Elapsed'] = pd.to_numeric(data['Elapsed'], errors='coerce')\n",
    "\n",
    "# Check for invalid rows and remove them\n",
    "data = data.dropna(subset=['Elapsed'])\n",
    "\n",
    "# Boxplot with custom figure size\n",
    "plt.figure(figsize=(6,3))\n",
    "\n",
    "# Create a boxplot for 'Elapsed' grouped by 'Technique'\n",
    "sns.boxplot(data=data, x=\"Technique\", y=\"Elapsed\", palette=\"Set3\", width=0.4)\n",
    "\n",
    "# Adding titles and labels for the boxplot\n",
    "plt.title('Boxplot of Elapsed Time for each forecasting technique applied to all the avaliable voice flows', fontsize=10)\n",
    "plt.xlabel('Technique', fontsize=12)\n",
    "plt.ylabel('Elapsed Time (seconds)', fontsize=12)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the boxplot\n",
    "plt.savefig('elapsed times of  box plot', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
